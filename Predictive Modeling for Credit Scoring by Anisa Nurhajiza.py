# -*- coding: utf-8 -*-
"""Predictive Modeling for Credit Scoring by Anisa Nurhajiza.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZVFqG7EisWZmzzwvyMrZquOY5B74Z3GA

# **PREDICTIVE MODELING FOR CREDIT SCORING**  
### <span style="color:blue;">Final Project Data Scientist at ID/X Partners</span>  
**Author:** Anisa Nurhajiza

# **Business Understanding**

---



**Background Overview**

Multifinance companies face significant challenges in accurately assessing and managing credit risk, which is crucial for making informed business decisions and minimizing potential losses. Traditional methods often fail to capture the complexity of modern financial data, leading to suboptimal risk assessments and higher default rates.

**Objective**

Develop a robust machine learning model capable of predicting credit risk with high accuracy, thereby improving the decision-making process for loan approvals and reducing the incidence of default. The model should be interpretable to meet regulatory requirements and provide actionable insights.

**Data**

The dataset collected from 2007 to 2014 consists of various attributes that are crucial for assessing credit risk.


**Methods**

To build an effective credit risk prediction model, the following machine learning algorithms will be employed:


*   Logistic Regression: A baseline model that provides interpretability and insight into feature importance.
*   Decision Tree: A simple model that offers interpretability and can handle non-linear relationships.


Model performance will be evaluated using cross-validation and metrics AUC-ROC.

# **Data Understanding**

# **Import Data and Check Dimension Data**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/drive/My Drive/Rakamin Academy/IDX/loan_data_2007_2014.csv")
print('This dataset has %d rows dan %d columns.\n' % df.shape)
df.head()

"""Checking whether the rows in the data represent an individual."""

# Check the number of unique values in the id column and the number of unique values indicating individual identities (member_id)
num_rows = df['id'].nunique()
unique_count = df['member_id'].nunique()

# Check if each row represents a single individual
if num_rows == unique_count:
    print("Each row represents a single individual.")
else:
    print("There are duplications in the data.")

# View dataset structure
df.info()

"""# **Features Engineering**"""

missing_columns = df.columns[df.isnull().all()]

# Menampilkan daftar kolom yang hanya berisi missing value
print(missing_columns)

# Defining columns will be dropped
cols_to_drop = [
    # unique id
    'Unnamed: 0',
    'id',
    'member_id',

    # free text
    'url',
    'desc',

    # others
    'zip_code',
    'mths_since_last_major_derog',
    'tot_coll_amt',
    'tot_cur_bal',
    'total_rev_hi_lim',

    # Expert judgment
    'sub_grade'
]

# Dropping columns that contain only missing values
df = df.dropna(axis=1, how='all')
df.info()

# Dropping columns listed in cols_to_drop
df.drop(cols_to_drop, axis=1, inplace=True)

# Displaying dataset information after dropping columns
print(df.info())

"""# Defining Target Variable"""

print(df['loan_status'].value_counts())

"""# Loan Status

1. **Current**: 224,226 loans. This indicates that the loan is active, and payments are being made on time according to the schedule. Borrowers are paying their installments without delays.  

2. **Fully Paid**: 184,739 loans. These loans have been fully repaid by the borrowers. No remaining payments are due.  

3. **Charged Off**: 42,475 loans. These loans are considered a loss by the lender. This typically occurs when the borrower is unable to repay the loan, and the lender decides to cease collection efforts.  

4. **Late (31-120 days)**: 6,900 loans. Loans with payments overdue for 31 to 120 days.  

5. **In Grace Period**: 3,146 loans. These loans are in the grace period. This is a timeframe after the payment due date during which the borrower has not yet been charged a late fee but is expected to make payment soon to avoid being marked as late.  

6. **Does not meet the credit policy. Status: Fully Paid**: 1,988 loans. Loans that did not meet the credit policy but have been fully repaid.  

7. **Late (16-30 days)**: 1,218 loans. These loans have payment delays ranging from 16 to 30 days. This indicates that borrowers are less than a month overdue on their installments.  

8. **Default**: 832 loans. These loans are in default status, meaning the borrower has failed to repay the loan and has made no further effort to settle the debt.  

9. **Does not meet the credit policy. Status: Charged Off**: 761 loans. Loans that did not meet the credit policy and are classified as a loss.  
"""

import numpy as np

# List of payment statuses considered as 'bad_status'
bad_status = [
    'Charged Off',
    'Default',
    'Does not meet the credit policy. Status: Charged off',
    'Late (31-120 days)'
]

# Adding the 'bad_flag' column to indicate whether the payment status is within 'bad_status'
df['bad_flag'] = np.where(df['loan_status'].isin(bad_status), 1, 0)

"""**bad_status** contains payment statuses considered problematic or unfavorable."""

# Calculating the percentage distribution of values in the 'loan_status' column to obtain the proportion of values in percentage form
loan_status_percentage = df['loan_status'].value_counts(normalize=True) * 100

# Displaying the percentage distribution of values in the 'loan_status' column
print(loan_status_percentage)

# Calculating the percentage distribution of values in the 'bad_flag' column
bad_flag_percentage = df['bad_flag'].value_counts(normalize=True) * 100

# Displaying the percentage distribution of values in the 'bad_flag' column
print(bad_flag_percentage)

# Calculating both counts and percentages of values in the 'bad_flag' column
bad_flag_counts = df['bad_flag'].value_counts()
bad_flag_percentage = df['bad_flag'].value_counts(normalize=True) * 100

# Combining counts and percentages into a single DataFrame
bad_flag_summary = pd.DataFrame({
    'Count': bad_flag_counts,
    'Percentage': bad_flag_percentage
})

# Displaying the summary
print(bad_flag_summary)

"""**0:** Approximately 89.23% of loans are categorized as non-problematic, indicating that the majority of loans in this dataset have a good payment status.  

**1:** Around 10.77% of loans fall into the problematic category (bad_status), including statuses like Charged Off, Default, Late (31-120 days), or Does not meet the credit policy: Charged Off.  
"""

# Dropping the 'loan_status' column
df.drop('loan_status', axis=1, inplace=True)

"""Removing the `loan_status` column after adding the `bad_flag` column is a common practice to simplify the dataset and focus on specific analyses. Hereâ€™s why:  

- **Data Simplification**: Removing `loan_status` makes the dataset more streamlined and manageable. The `bad_flag` column provides essential information about loan status in a concise format (1 for problematic loans, 0 for non-problematic loans).  

- **Redundancy**: Once the `bad_flag` column is created, most of the information in `loan_status` is already represented, making it redundant for analyses focused solely on loan performance.

# Data Cleaning
"""

df.head()

"""Column: term"""

# Displaying unique values from the 'term' column
unique_terms = df['term'].unique()
print(unique_terms)

# Removing additional text and converting the data type to float
df['term_int'] = df['term'].str.replace(' months', '').astype(float)
df.head()

# Dropping the 'term' column
df.drop('term', axis=1, inplace=True)

"""Column: emp_length"""

# Getting unique values from the 'emp_length' column
df['emp_length'].unique()

# Extracting numbers from the 'emp_length' column and converting them to float
df['emp_length_int'] = df['emp_length'].str.extract(r'(\d+)').astype(float)

# Dropping the 'emp_length' column
df.drop('emp_length', axis=1, inplace=True)

df.head()

"""Column: issue_d"""

# Converting 'issue_d' to date format
df['issue_d_date'] = pd.to_datetime(df['issue_d'], format='%b-%y')
df.head()

# Calculating the number of months since 'issue_d' until the reference date
from dateutil.relativedelta import relativedelta

# Assuming 'issue_d_date' column exists in the DataFrame and is in datetime format
reference_date = pd.to_datetime('2017-12-01')

# Define a function to calculate months difference
def months_difference(issue_date, ref_date):
    return relativedelta(ref_date, issue_date).years * 12 + relativedelta(ref_date, issue_date).months

# Apply the function to calculate months difference
df['mths_since_issue_d'] = df['issue_d_date'].apply(lambda x: months_difference(x, reference_date))

df.head()

"""**mths_since_issue_d**
- This column calculates the duration in months between two dates: `issue_d_date` and the reference date `2017-12-01`.  
- This duration can be used for various time-based analyses, such as studying behavioral patterns based on the age of an account or event.  
"""

# Descriptive statistics for the 'mths_since_issue_d' column
print(df['mths_since_issue_d'].describe())

# Dropping the columns 'issue_d' and 'issue_d_date'
df.drop(['issue_d', 'issue_d_date'], axis=1, inplace=True)

"""Column: earliest_cr_line"""

print(df['earliest_cr_line'].head())

# Converting 'earliest_cr_line' to date format
df['earliest_cr_line_date'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
print(df['earliest_cr_line_date'].head())

import pandas as pd
from dateutil.relativedelta import relativedelta

# Reference date
reference_date = pd.to_datetime('2017-12-01')

# Function to calculate months difference
def months_difference(issue_date, ref_date):
    if pd.isna(issue_date):  # Handle missing dates
        return None
    return relativedelta(ref_date, issue_date).years * 12 + relativedelta(ref_date, issue_date).months

# Ensure 'earliest_cr_line_date' is in datetime format
df['earliest_cr_line_date'] = pd.to_datetime(df['earliest_cr_line_date'], errors='coerce')

# Apply the function to calculate months difference
df['mths_since_earliest_cr_line'] = df['earliest_cr_line_date'].apply(lambda x: months_difference(x, reference_date))

print(df['mths_since_earliest_cr_line'].head())

"""**mths_since_earliest_cr_line**  

A column that represents the number of months elapsed from the date in `earliest_cr_line_date` to the reference date `2017-12-01`. These values indicate the duration in months from that date to the reference date.  
"""

# Descriptive statistics for the 'mths_since_earliest_cr_line' column
print(df['mths_since_earliest_cr_line'].describe())

# Displaying rows with 'mths_since_earliest_cr_line' values less than 0
print(df[df['mths_since_earliest_cr_line'] < 0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head())

"""The negative values occurred because the Python function misinterpreted the year `62` as `2062` instead of `1962`.

To address this, further preprocessing can be done to correct the year `2062` to `1962`. However, in this case, negative values are simply replaced with the maximum value of the feature, as they represent older data (from the 1900s), making this adjustment reasonable.
"""

# Replacing values in 'mths_since_earliest_cr_line' less than 0 with the maximum value
df.loc[df['mths_since_earliest_cr_line'] < 0, 'mths_since_earliest_cr_line'] = df['mths_since_earliest_cr_line'].max()

# Dropping the columns 'earliest_cr_line' and 'earliest_cr_line_date'
df.drop(['earliest_cr_line', 'earliest_cr_line_date'], axis=1, inplace=True)

"""Column: last_pymnt_d"""

# Converting 'last_paymnt_d' to date format
df['last_pymnt_d_date'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%y')

# Calculating the number of months since 'last_payment_d' until the reference date
import pandas as pd
from dateutil.relativedelta import relativedelta

# Reference date
reference_date = pd.to_datetime('2017-12-01')

# Function to calculate months difference
def months_difference(issue_date, ref_date):
    if pd.isna(issue_date):  # Handle missing dates
        return None
    return relativedelta(ref_date, issue_date).years * 12 + relativedelta(ref_date, issue_date).months

# Ensure 'last_pymnt_d_date' is in datetime format
df['last_pymnt_d_date'] = pd.to_datetime(df['last_pymnt_d_date'], errors='coerce')

# Apply the function to calculate months difference
df['mths_since_last_pymnt_d'] = df['last_pymnt_d_date'].apply(lambda x: months_difference(x, reference_date))

print(df['mths_since_last_pymnt_d'].head())

# Descriptive statistics for the 'mths_since_last_pymnt_d' column
print(df['mths_since_last_pymnt_d'].describe())

# Dropping the columns 'last_pymnt_d' and 'last_pymnt_d_date'
df.drop(['last_pymnt_d', 'last_pymnt_d_date'], axis=1, inplace=True)

"""Column: next_pymnt_d"""

# Converting 'next_pymnt_d' to date format
df['next_pymnt_d_date'] = pd.to_datetime(df['next_pymnt_d'], format='%b-%y')

# Calculating the number of months since 'next_pymnt_d' until the reference date
import pandas as pd
from dateutil.relativedelta import relativedelta

# Reference date
reference_date = pd.to_datetime('2017-12-01')

# Function to calculate months difference
def months_difference(issue_date, ref_date):
    if pd.isna(issue_date):  # Handle missing dates
        return None
    return relativedelta(ref_date, issue_date).years * 12 + relativedelta(ref_date, issue_date).months

# Ensure 'next_pymnt_d_date' is in datetime format
df['next_pymnt_d_date'] = pd.to_datetime(df['next_pymnt_d_date'], errors='coerce')

# Apply the function to calculate months difference
df['mths_since_next_pymnt_d'] = df['next_pymnt_d_date'].apply(lambda x: months_difference(x, reference_date))

print(df['mths_since_next_pymnt_d'].head())

# Descriptive statistics for the 'mths_since_next_pymnt_d' column
print(df['mths_since_next_pymnt_d'].describe())

# Dropping the columns 'next_pymnt_d' and 'next_pymnt_d_date'
df.drop(['next_pymnt_d', 'next_pymnt_d_date'], axis=1, inplace=True)

"""Column: last_credit_pull_d"""

# Converting 'last_credit_pull_d' to date format
df['last_credit_pull_d_date'] = pd.to_datetime(df['last_credit_pull_d'], format='%b-%y')

# Calculating the number of months since 'last_credit_pull_d' until the reference date
import pandas as pd
from dateutil.relativedelta import relativedelta

# Reference date
reference_date = pd.to_datetime('2017-12-01')

# Function to calculate months difference
def months_difference(issue_date, ref_date):
    if pd.isna(issue_date):  # Handle missing dates
        return None
    return relativedelta(ref_date, issue_date).years * 12 + relativedelta(ref_date, issue_date).months

# Ensure 'last_credit_pull_d_date' is in datetime format
df['last_credit_pull_d_date'] = pd.to_datetime(df['last_credit_pull_d_date'], errors='coerce')

# Apply the function to calculate months difference
df['mths_since_last_credit_pull_d'] = df['last_credit_pull_d_date'].apply(lambda x: months_difference(x, reference_date))

print(df['mths_since_last_credit_pull_d'].head())

# Descriptive statistics for the 'mths_since_last_credit_pull_d' column
print(df['mths_since_last_credit_pull_d'].describe())

# Dropping the columns 'last_credit_pull_d' and 'last_credit_pull_d_date'
df.drop(['last_credit_pull_d', 'last_credit_pull_d_date'], axis=1, inplace=True)

df.head()

"""# **Exploratory Data Analysis**

# Check Correlation
"""

import seaborn as sns
import matplotlib.pyplot as plt

numeric_df = df.select_dtypes(include=['int64', 'float64'])

# Hitung korelasi antar kolom numerik
correlation_matrix = numeric_df.corr()

# Buat heatmap korelasi
plt.figure(figsize=(24, 20))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 10}, linewidths=0.5)
plt.title('Correlation Heatmap', fontsize=20)
plt.xticks(fontsize=14, rotation=90, ha='right')
plt.yticks(fontsize=14, rotation=0)
plt.tight_layout()
plt.show()

"""# Check Cardinality Data

Categorical Data
"""

# Dropping features with high correlation
df.drop(['funded_amnt', 'funded_amnt_inv', 'installment', 'out_prncp_inv', 'total_rec_prncp',
         'total_pymnt', 'total_pymnt_inv', 'total_rec_int', 'collection_recovery_fee',
         'last_pymnt_amnt', 'mths_since_last_pymnt_d', 'mths_since_next_pymnt_d',
         'mths_since_last_credit_pull_d'], axis=1, inplace=True)

# The number of unique values for features with object data type
print(df.select_dtypes(include='object').nunique())

# Dropping 3 features
df.drop(['emp_title', 'title', 'application_type'], axis=1, inplace=True)
df.head()

"""Numerical Data"""

# The number of unique values for features with non-object data type
print(df.select_dtypes(exclude='object').nunique())

# Dropping the 'policy_code' feature
df.drop(['policy_code'], axis=1, inplace=True)

# Loop through each feature with object data type
for col in df.select_dtypes(include=['object']).columns.tolist():
    # Display the distribution of unique values for each feature
    print("Distribution of unique values for feature", col)
    print(df[col].value_counts(normalize=True) * 100)
    print('\n')

"""Pada tahap ini, fitur yang didominasi oleh satu nilai secara signifikan akan dihapus dari dataset. Hal ini dilakukan untuk menghilangkan fitur yang tidak memberikan variasi atau informasi tambahan dalam proses pemodelan"""

# Dropping the 'pymnt_plan' feature from the dataset
df.drop('pymnt_plan', axis=1, inplace=True)
df.info()

"""# Univariate Analysis"""

# identifying the categorical variables
cat_var = df.select_dtypes(include=["object"]).columns
print(cat_var)

# plotting bar chart for each categorical variable
plt.style.use("ggplot")

for column in cat_var:
    plt.figure(figsize=(20, 4))
    plt.subplot(121)
    df[column].value_counts().plot(kind="bar", color="HotPink")
    plt.xlabel(column)
    plt.ylabel("number of customers")
    plt.title(column)

"""Numerical Data"""

# Identifying the numerical variables
num_var = df.select_dtypes(include = np.number)
num_var.head()

# Plotting histogram for each numeric variable
plt.style.use("ggplot")

# Filtering columns with dtype other than 'object'
for column in df.select_dtypes(exclude='object').columns.tolist():
    # Calculating descriptive statistics
    average = df[column].mean()
    median = df[column].median()
    mode = df[column].mode().iloc[0] # Taking the first mode (as there can be multiple modes)
    std = df[column].std()

    # Creating subplot for histogram
    plt.figure(figsize=(15, 4))

    plt.subplot(121)
    sns.distplot(df[column], kde=True, color='Violet')
    plt.axvline(average, color='r', linestyle='solid', linewidth=3, label='Mean')
    plt.axvline(median, color='b', linestyle='dotted', linewidth=3, label='Median')
    plt.axvline(mode, color='purple', linestyle='dashed', linewidth=3, label='Mode')
    plt.title(column)

    # Displaying summary statistics
    print('Summary statistics of {column}'.format(column=column))
    print('Mean: ', "%.2f" % average)
    print('Standard deviation: ', "%.2f" % std)
    print('Median: ', "%.2f" % median)
    print('Mode: ', mode)

"""# Bivariate Analysis

Categorical Data
"""

custom_palette = sns.color_palette(["#ff69b4", "#40e0d0"])
plt.style.use("default")
sns.set_style("whitegrid")

# Looping to create countplot visualizations
for column in cat_var:
    plt.figure(figsize=(20, 4))
    plt.subplot(121)
    sns.countplot(x=df[column], hue=df["bad_flag"], palette=custom_palette)
    plt.title(column)
    plt.xticks(rotation=90)
    plt.show()

"""# **Data Preprocessing**

# Check Missing Value
"""

# Calculating the percentage of missing values for each feature
missing_percentage = df.isnull().sum() * 100 / df.shape[0]

# Selecting only features with missing values
missing_values = missing_percentage[missing_percentage > 0].sort_values(ascending=False)

# Displaying the results
print("Percentage of missing values for each feature:")
print(missing_values)

df.drop('mths_since_last_record', axis = 1, inplace = True)

"""# Data Imputation"""

df['annual_inc'].fillna(df['annual_inc'].median(), inplace=True)
df['mths_since_earliest_cr_line'].fillna(0, inplace=True)
df['acc_now_delinq'].fillna(0, inplace=True)
df['total_acc'].fillna(0, inplace=True)
df['pub_rec'].fillna(0, inplace=True)
df['open_acc'].fillna(0, inplace=True)
df['inq_last_6mths'].fillna(0, inplace=True)
df['delinq_2yrs'].fillna(0, inplace=True)
df['collections_12_mths_ex_med'].fillna(0, inplace=True)
df['revol_util'].fillna(0, inplace=True)
df['emp_length_int'].fillna(0, inplace=True)
df['mths_since_last_delinq'].fillna(-1, inplace=True)

"""# Label Encoding"""

from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()

for column in cat_var:
  df[column] = label.fit_transform(df[column])

df.head()

"""# Handling Imbalanced Data"""

# Handling Imbalance Data
from imblearn.over_sampling import SMOTE

# Misalkan fitur (X) dan Target (y)
X = df.drop('bad_flag', axis = 1)
y = df['bad_flag']

oversampling = SMOTE(random_state = 12, sampling_strategy=1)

# Convert all columns to numeric data type
X = X.apply(pd.to_numeric, errors='coerce')

# Fit the over sampling
X,y = oversampling.fit_resample(X, y)

y.value_counts()

"""# Train Test Split"""

from sklearn.model_selection import train_test_split

# Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Displaying the shape of the training and test data
print("Shape of training data:", X_train.shape)
print("Shape of test data:", X_test.shape)

"""# Standardization"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

# After standardization
X_train_scaled.head()

"""# **Data Modeling**

# Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train_scaled, y_train)

y_train_pred = logreg.predict(X_train_scaled)
y_test_pred = logreg.predict(X_test)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# Initialize Decision Tree model
tree_clf = DecisionTreeClassifier()

# Train the model with scaled training data
tree_clf.fit(X_train_scaled, y_train)

# Predict on training and test data
y_train_pred_tree = tree_clf.predict(X_train_scaled)
y_test_pred_tree = tree_clf.predict(X_test)

"""# **Evaluation**  
## Receiver Operating Characteristic (ROC) Curve  

The ROC curve (AUC) evaluates a model's ability to distinguish between classes as the decision threshold varies.  

- The vertical axis represents the True Positive Rate (TPR) or Sensitivity.  
- The horizontal axis represents the False Positive Rate (FPR).  

The ROC curve illustrates the balance between TPR and FPR at different classification thresholds. Ideally, the curve should move toward the top-left corner, indicating high TPR and low FPR.

# ROC-AUC for Logistic Regression Training Model
"""

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Predict probabilities for the test set
y_prob = logreg.predict_proba (X_train_scaled) [:, 1]

# Calculate the ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_train, y_prob)
roc_auc = roc_auc_score (y_train, y_prob)

# Plot the ROC curve
plt.figure(figsize=(5, 4))
plt.plot(fpr, tpr, color='#FF69B4', lw=2, label=f'ROC Curve (AUC= {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='DeepSkyBlue', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve: Logistic Regression Training Model')
plt.legend(loc='lower right')
plt.show()

"""# ROC-AUC for Logistic Regression Testing Model"""

# Predict probabilities for testing data
y_prob_logreg_test = logreg.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC for testing data
fpr_logreg_test, tpr_logreg_test, thresholds_logreg_test = roc_curve(y_test, y_prob_logreg_test)
roc_auc_logreg_test = roc_auc_score(y_test, y_prob_logreg_test)

# Plot ROC curve for testing data
plt.figure(figsize=(5, 4))
plt.plot(fpr_logreg_test, tpr_logreg_test, color='#FF69B4', lw=2, label=f'ROC Curve (AUC= {roc_auc_logreg_test:.2f})')
plt.plot([0, 1], [0, 1], color='DeepSkyBlue', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve: Logistic Regression Testing Model')
plt.legend(loc='lower right')
plt.show()

"""# Receiver Operating Characteristic (ROC) Curve for Logistic Regression Model

1. **ROC Curve: Logistic Regression (Training)**
   - The AUC is 0.91, indicating excellent performance on the training data. The closer the value is to 1, the better the model distinguishes between positive and negative classes.
   - The curve approaching the top-left corner shows high True Positive Rate and low False Positive Rate across various classification thresholds.

2. **ROC Curve: Logistic Regression (Testing)**
   - The AUC is 0.66, significantly lower than on the training data.
   - This suggests the logistic regression model does not generalize well to unseen data (testing data).
   - The curve is closer to the diagonal line, indicating performance similar to random guessing, which is lower than on the training data.

Overall, the graphs suggest potential overfitting on the training data, as there is a significant drop in AUC when testing on new data. The model performs well on the training data but fails to maintain the same performance on new data. The next step is to reduce overfitting and adjust model parameters to improve AUC on testing data.

# ROC-AUC for Decision Tree Training Model
"""

# Predict probabilities for training data
y_prob_tree = tree_clf.predict_proba(X_train_scaled)[:, 1]

# Calculate ROC curve and AUC
fpr_tree, tpr_tree, thresholds_tree = roc_curve(y_train, y_prob_tree)
roc_auc_tree = roc_auc_score(y_train, y_prob_tree)

# Plot ROC curve
plt.figure(figsize=(5, 4))
plt.plot(fpr_tree, tpr_tree, color='#FF69B4', lw=2, label=f'ROC Curve (AUC= {roc_auc_tree:.2f})')
plt.plot([0, 1], [0, 1], color='DeepSkyBlue', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve: Decision Tree Training Model')
plt.legend(loc='lower right')
plt.show()

"""# ROC-AUC for Decision Tree Testing Model"""

# Predict probabilities for testing data
y_prob_tree_test = tree_clf.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC for testing data
fpr_tree_test, tpr_tree_test, thresholds_tree_test = roc_curve(y_test, y_prob_tree_test)
roc_auc_tree_test = roc_auc_score(y_test, y_prob_tree_test)

# Plot ROC curve for testing data
plt.figure(figsize=(5, 4))
plt.plot(fpr_tree_test, tpr_tree_test, color='#FF69B4', lw=2, label=f'ROC Curve (AUC= {roc_auc_tree_test:.2f})')
plt.plot([0, 1], [0, 1], color='DeepSkyBlue', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve: Decision Tree Testing Model')
plt.legend(loc='lower right')
plt.show()

"""# Receiver Operating Characteristic (ROC) Curve for Decision Tree Model

1. **ROC Curve for Training Data**  
   - The AUC is 1.00, indicating perfect performance on the training data.  
   - A high AUC shows the model can distinguish well between positive and negative classes on the training data.  

2. **ROC Curve for Testing Data**  
   - The AUC is 0.57, indicating that the model performs much worse on the testing data compared to the training data.  
   - An AUC close to 0.5 suggests the model's ability to differentiate between classes is almost equivalent to random guessing.  

The significant difference between AUC on training and testing data suggests overfitting. This occurs when the model is too complex and fits the training data too closely, failing to generalize well to unseen data.

# **Conclusion**

**Conclusion**  
The evaluation of both the Logistic Regression and Decision Tree models highlights important insights regarding model performance and generalization.  

- For **Logistic Regression**, the model performs excellently on the training data (AUC = 0.91), but struggles to generalize to new, unseen data (AUC = 0.66), suggesting potential overfitting. This indicates that while the model fits the training data well, it fails to maintain the same level of accuracy on testing data. Further adjustments, such as regularization or parameter tuning, are necessary to improve its performance on unseen data.  

- For the **Decision Tree**, the model shows perfect performance on training data (AUC = 1.00), but its performance on the testing data is significantly worse (AUC = 0.57), indicating strong overfitting. This suggests that the decision tree has become overly complex and has learned the noise in the training data rather than the underlying patterns. Simplifying the model, using pruning techniques, or applying cross-validation could help reduce overfitting and improve generalization.

Overall, both models show signs of overfitting, and further steps are needed to enhance their ability to generalize to new data, ensuring more reliable predictions in real-world scenarios.
"""